{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8933712,
          "sourceType": "datasetVersion",
          "datasetId": 2129637
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"emirhanai/emotion-prediction-with-semi-supervised-learning\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "e2PoIt8vSh78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df87f100-3c2f-490f-a20e-0c891e1cd452"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/emirhanai/emotion-prediction-with-semi-supervised-learning/versions/32\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using Q-NLP\n"
      ],
      "metadata": {
        "id": "GiOd8G_VSh7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For backwards compatibiliy\n",
        "> qml.qnn.KerasLayer  \n",
        "\n",
        "KerasLayer requires a Keras version lower than 3. For instructions on running with Keras 2\n",
        "But, tensorflow automatically installs Keras 3\n",
        "\n",
        "Install tf_keras: `pip install tf_keras`  \n",
        "Change env variables to use legacy keras"
      ],
      "metadata": {
        "id": "nmpDoJDXSh8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ],
      "metadata": {
        "id": "J3dCzvwMSh8A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries\n",
        "\n",
        "The libraries required for for implementation"
      ],
      "metadata": {
        "id": "u9Y_-z0CSh8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow keras -y\n",
        "!pip install tensorflow==2.9.0 keras==2.9.0\n",
        "!pip install tensorflow-quantum==0.7.2\n",
        "!pip install pennylane\n",
        "!pip install tf_keras\n",
        "!pip install cirq==0.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LgexTZiybof3",
        "outputId": "2c74ddd8-2c6e-4c85-d26f-749ca732d447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.1\n",
            "Uninstalling tensorflow-2.17.1:\n",
            "  Successfully uninstalled tensorflow-2.17.1\n",
            "Found existing installation: keras 2.9.0\n",
            "Uninstalling keras-2.9.0:\n",
            "  Successfully uninstalled keras-2.9.0\n",
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting keras==2.9.0\n",
            "  Using cached keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.12.1)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.7)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2021.5.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m915.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.73.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cbef43c76ff64987abf7782940f3de65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-quantum==0.7.2 (from versions: 0.7.3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-quantum==0.7.2\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.39.0)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.8.8)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.15.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.7.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Requirement already satisfied: pennylane-lightning>=0.39 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.39.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2021.5.30)\n",
            "Requirement already satisfied: tf_keras in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting tensorflow<2.18,>=2.17 (from tf_keras)\n",
            "  Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.6.3)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow<2.18,>=2.17->tf_keras)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow<2.18,>=2.17->tf_keras)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.68.1)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf_keras)\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.2.0 (from tensorflow<2.18,>=2.17->tf_keras)\n",
            "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf_keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (2021.5.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (3.7)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (0.1.2)\n",
            "Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m751.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Using cached keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flatbuffers, tensorboard-data-server, protobuf, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.0\n",
            "    Uninstalling tensorflow-2.9.0:\n",
            "      Successfully uninstalled tensorflow-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.5 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-24.3.25 keras-3.7.0 protobuf-4.25.5 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.1\n",
            "Requirement already satisfied: cirq==0.13.1 in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cirq-aqt==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: cirq-core==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: cirq-google==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: cirq-ionq==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: cirq-pasqal==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: cirq-rigetti==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: cirq-web==0.13.1 in /usr/local/lib/python3.10/dist-packages (from cirq==0.13.1) (0.13.1)\n",
            "Requirement already satisfied: requests~=2.18 in /usr/local/lib/python3.10/dist-packages (from cirq-aqt==0.13.1->cirq==0.13.1) (2.32.3)\n",
            "Requirement already satisfied: duet~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (0.2.9)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (3.8.0)\n",
            "Requirement already satisfied: networkx~=2.4 in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (2.8.8)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (1.13.1)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (2.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from cirq-core==0.13.1->cirq==0.13.1) (4.12.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.10/dist-packages (from cirq-google==0.13.1->cirq==0.13.1) (4.25.5)\n",
            "Requirement already satisfied: attrs~=20.3.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (20.3.0)\n",
            "Requirement already satisfied: certifi~=2021.5.30 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (2021.5.30)\n",
            "Requirement already satisfied: h11~=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (0.9.0)\n",
            "Requirement already satisfied: httpcore~=0.11.1 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (0.11.1)\n",
            "Requirement already satisfied: httpx~=0.15.5 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (0.15.5)\n",
            "Requirement already satisfied: idna~=2.10 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (2.10)\n",
            "Requirement already satisfied: iso8601~=0.1.14 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (0.1.16)\n",
            "Requirement already satisfied: pydantic~=1.8.2 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (1.8.2)\n",
            "Requirement already satisfied: pyjwt~=1.7.1 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil~=2.8.1 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (2.8.2)\n",
            "Requirement already satisfied: qcs-api-client~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (0.8.0)\n",
            "Requirement already satisfied: retrying~=1.3.3 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (1.3.4)\n",
            "Requirement already satisfied: rfc3339~=6.2 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (6.2)\n",
            "Requirement already satisfied: rfc3986~=1.5.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (1.5.0)\n",
            "Requirement already satisfied: six~=1.16.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (1.16.0)\n",
            "Requirement already satisfied: sniffio~=1.2.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (1.2.0)\n",
            "Requirement already satisfied: toml~=0.10.2 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (0.10.2)\n",
            "Requirement already satisfied: pyquil~=3.0.0 in /usr/local/lib/python3.10/dist-packages (from cirq-rigetti==0.13.1->cirq==0.13.1) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (1.66.0)\n",
            "Collecting protobuf>=3.13.0 (from cirq-google==0.13.1->cirq==0.13.1)\n",
            "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (2.27.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (1.48.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==0.13.1->cirq==0.13.1) (3.2.0)\n",
            "Requirement already satisfied: lark<0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (0.11.3)\n",
            "Requirement already satisfied: retry<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (0.9.2)\n",
            "Requirement already satisfied: rpcq<4.0.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (3.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.18->cirq-aqt==0.13.1->cirq==0.13.1) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.18->cirq-aqt==0.13.1->cirq==0.13.1) (2.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cirq-core==0.13.1->cirq==0.13.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->cirq-core==0.13.1->cirq==0.13.1) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->cirq-core==0.13.1->cirq==0.13.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.14.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.14.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.14.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (4.9)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry<0.10.0,>=0.9.2->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (4.4.2)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry<0.10.0,>=0.9.2->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (1.11.0)\n",
            "Requirement already satisfied: msgpack<2.0,>=0.6 in /usr/local/lib/python3.10/dist-packages (from rpcq<4.0.0,>=3.6.0->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (1.1.0)\n",
            "Requirement already satisfied: python-rapidjson in /usr/local/lib/python3.10/dist-packages (from rpcq<4.0.0,>=3.6.0->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (1.20)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from rpcq<4.0.0,>=3.6.0->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (24.0.1)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from rpcq<4.0.0,>=3.6.0->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (0.18.6)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.14.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.13.1->cirq==0.13.1) (0.6.1)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->rpcq<4.0.0,>=3.6.0->pyquil~=3.0.0->cirq-rigetti==0.13.1->cirq==0.13.1) (0.2.12)\n",
            "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4b0456cbc42b472293f64708b2de992d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries\n",
        "Importing essential libraries for the project"
      ],
      "metadata": {
        "id": "UvBy9rQ7xpls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as p_np\n",
        "\n",
        "from pennylane.templates.state_preparations import MottonenStatePreparation\n",
        "from pennylane.templates.layers import StronglyEntanglingLayers\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-10-25T23:01:01.066187Z",
          "iopub.status.busy": "2022-10-25T23:01:01.065826Z",
          "iopub.status.idle": "2022-10-25T23:01:01.073487Z",
          "shell.execute_reply": "2022-10-25T23:01:01.07248Z",
          "shell.execute_reply.started": "2022-10-25T23:01:01.066157Z"
        },
        "id": "C-rU0M1gxplz",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the dataset\n",
        "Import the dataset and prepare the dataframe"
      ],
      "metadata": {
        "id": "O72fvaQOSh8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load our data\n",
        "data = pd.read_csv(\"/tweet_emotions.csv\")\n",
        "#plt.style.use('fivethirtyeight')"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T21:50:24.044609Z",
          "iopub.status.busy": "2022-10-25T21:50:24.044239Z",
          "iopub.status.idle": "2022-10-25T21:50:24.100672Z",
          "shell.execute_reply": "2022-10-25T21:50:24.099691Z",
          "shell.execute_reply.started": "2022-10-25T21:50:24.044577Z"
        },
        "trusted": true,
        "id": "d1nRCm1BSh8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "9dd7a0df-7c3e-433d-a800-ffe1995e0f72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'P:\\\\QuantumML\\\\QML_project\\\\archive\\\\tweet_emotions.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e656ec48dd55>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P:\\\\QuantumML\\\\QML_project\\\\archive\\\\tweet_emotions.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.style.use('fivethirtyeight')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'P:\\\\QuantumML\\\\QML_project\\\\archive\\\\tweet_emotions.csv'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA\n",
        "Exploratory data analysis of the dataset"
      ],
      "metadata": {
        "id": "3jt_5afsSh8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "5eI_02ZiSh8D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape of the dataset"
      ],
      "metadata": {
        "id": "C1kBqf8mSh8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "-5fmiGqWSh8D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns"
      ],
      "metadata": {
        "id": "xSqDMF0-Sh8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "4mo2ov7gSh8D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "Z9XImf3GSh8D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "PFetbGb0Sh8D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "No null values"
      ],
      "metadata": {
        "id": "u04-U0__Sh8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "0fwVsyFjxpl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y=data[\"sentiment\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T21:50:25.720089Z",
          "iopub.status.busy": "2022-10-25T21:50:25.719714Z",
          "iopub.status.idle": "2022-10-25T21:50:25.984937Z",
          "shell.execute_reply": "2022-10-25T21:50:25.984Z",
          "shell.execute_reply.started": "2022-10-25T21:50:25.720055Z"
        },
        "id": "9KW79cB1zXqq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleanup and Prepare"
      ],
      "metadata": {
        "id": "BBMi7ePjxpl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original paper proposed using `RegexpStemmer` and remove expressions using  \n",
        "```snowball = RegexpStemmer('ing$|s$|e$|able$', min=4)```\n",
        "\n",
        "We propose a `PorterStemmer` and  `WordNetLemmatizer` based approach.\n",
        "    \n",
        " `PorterStemmer` removes the suffix of the word and creates stem word.  \n",
        "  `WordNetLemmatizer` leverages WordNet, a lexical database for the English Language to reduce words to base or dictionary forms."
      ],
      "metadata": {
        "id": "EBXqfhMlSh8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "00HrCY_LSh8E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Initialize stop words, stemmer, and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "porter_stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Process the content\n",
        "data[\"content\"] = data[\"content\"].apply(lambda x: \" \".join([\n",
        "    lemmatizer.lemmatize(porter_stemmer.stem(re.sub(r'[^\\w\\d]', \"\", word)))\n",
        "    for word in x.lower().split() if word not in stop_words\n",
        "]))\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "X = data[\"content\"]\n",
        "tokenize = Tokenizer()\n",
        "tokenize.fit_on_texts(X)\n",
        "\n",
        "max_length = 64\n",
        "vocab_size = len(tokenize.word_index) + 1\n",
        "X = pad_sequences(tokenize.texts_to_sequences(X), maxlen=max_length, padding=\"post\")"
      ],
      "metadata": {
        "id": "C9L5WIXzSh8E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemmed and Lemmatized data"
      ],
      "metadata": {
        "id": "kB4IJtSwSh8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "wQRarq3ESh8E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenized and encoded data"
      ],
      "metadata": {
        "id": "OLBm0T5PSh8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "SMYDvSiqSh8E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of the Model  \n",
        "The model architecture consists of several layers that are sequentially connected to form a neural network.   \n",
        "Architecture:\n",
        "\n",
        "**Quantum Layer (qlayer)**: This layer is implemented using the qnode function. It takes in two inputs: inputs and weights. The inputs represent the input data, and the weights represent the learnable parameters of the quantum layer.  \n",
        "Inside the qnode function:  \n",
        ">The `AngleEmbedding` template is used to encode the input data onto the quantum state.  \n",
        "  \n",
        ">The `StronglyEntanglingLayers` template is used to apply a series of parameterized quantum gates to the quantum state.  \n",
        "  \n",
        ">Finally, the expectation values of the `Pauli-Z` operators on qubits 0 and 1 are computed using `qml.expval`.  \n",
        "\n",
        "The output of the quantum layer is a tuple of these two expectation values.\n",
        "\n",
        "**Dense Layer (clayer1)**: This is a traditional dense layer implemented using the **Dense** class `from tf.keras.layers`. It takes in the output of the quantum layer as input and applies a linear transformation followed by an activation function. The number of neurons in this layer is 2.\n",
        "\n",
        "**Dense Layer (clayer2)**: This is another dense layer similar to clayer1.\n",
        "\n",
        "**Dense Layer (clayer3)**: This is the final dense layer of the model. It takes in the output of clayer2 as input and applies a linear transformation followed by a *softmax* activation function. The number of neurons in this layer is 13, which corresponds to the number of classes in the classification task.\n",
        "\n",
        "The model is compiled using the stochastic gradient descent (SGD) optimizer with a learning rate of 0.8, and the mean absolute error (MAE) loss function is used for training the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "OsEseTv2Sh8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def qnode(inputs, weights):\n",
        "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n",
        "\n",
        "weight_shapes = {\"weights\": (3, n_qubits, 3)}\n",
        "\n",
        "qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=2)\n",
        "clayer1 = tf.keras.layers.Dense(2)\n",
        "clayer2 = tf.keras.layers.Dense(2)\n",
        "clayer3 = tf.keras.layers.Dense(13, activation=\"softmax\")\n",
        "model = tf.keras.models.Sequential([clayer1, qlayer, clayer2, clayer3])\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.8)\n",
        "model.compile(opt, loss='mae')"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:42:32.059753Z",
          "iopub.status.busy": "2022-10-25T22:42:32.05938Z",
          "iopub.status.idle": "2022-10-25T22:42:32.085463Z",
          "shell.execute_reply": "2022-10-25T22:42:32.08459Z",
          "shell.execute_reply.started": "2022-10-25T22:42:32.059721Z"
        },
        "trusted": true,
        "id": "0ldxYDz2Sh8E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of Label"
      ],
      "metadata": {
        "id": "7-W0fsabSh8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data['sentiment']"
      ],
      "metadata": {
        "id": "wIzWQXdBSh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Y.value_counts()"
      ],
      "metadata": {
        "id": "rzay-jD9Sh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "EWL2mPobSh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "func = LabelEncoder()\n",
        "Y = func.fit_transform(Y)\n",
        "Y"
      ],
      "metadata": {
        "id": "BV9RVljJSh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tf.convert_to_tensor(Y)"
      ],
      "metadata": {
        "id": "6l2RaOjTSh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tf.one_hot(Y, depth=1)"
      ],
      "metadata": {
        "id": "7oHVJ70cSh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X shape = {X.shape}\")\n",
        "print(f\"Y shape = {Y.shape}\")"
      ],
      "metadata": {
        "id": "mC_WaiAQSh8F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Fitting"
      ],
      "metadata": {
        "id": "LM-1ztZaSh8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y, epochs=5, batch_size=256, shuffle=True,steps_per_epoch=1,validation_split=0.2)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:42:44.081903Z",
          "iopub.status.busy": "2022-10-25T22:42:44.080888Z",
          "iopub.status.idle": "2022-10-25T22:47:06.046765Z",
          "shell.execute_reply": "2022-10-25T22:47:06.045818Z",
          "shell.execute_reply.started": "2022-10-25T22:42:44.081856Z"
        },
        "trusted": true,
        "id": "vohq81LLSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of Prediction Data and Prediction Equation"
      ],
      "metadata": {
        "id": "aWycKMc2Sh8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"The loved the movie, the story was amazing and the acting was great.\"\n",
        "tokenize.fit_on_texts(a)\n",
        "\n",
        "max_length = 64\n",
        "vocab_size = len(tokenize.word_index) + 1\n",
        "a = pad_sequences(tokenize.texts_to_sequences(a), maxlen=max_length, padding=\"post\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:59:03.573662Z",
          "iopub.status.busy": "2022-10-25T22:59:03.573262Z",
          "iopub.status.idle": "2022-10-25T22:59:03.649573Z",
          "shell.execute_reply": "2022-10-25T22:59:03.648598Z",
          "shell.execute_reply.started": "2022-10-25T22:59:03.57363Z"
        },
        "trusted": true,
        "id": "MZXLccsOSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant(a)\n",
        "prediction = model.predict(a)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:59:03.65185Z",
          "iopub.status.busy": "2022-10-25T22:59:03.65137Z",
          "iopub.status.idle": "2022-10-25T22:59:04.536305Z",
          "shell.execute_reply": "2022-10-25T22:59:04.535296Z",
          "shell.execute_reply.started": "2022-10-25T22:59:03.651814Z"
        },
        "trusted": true,
        "id": "7bWbUo6DSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ap = []\n",
        "for i in range(len(prediction)):\n",
        "    ap.append(np.argmax(prediction[i]))\n",
        "np.argmax(ap)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:59:04.540065Z",
          "iopub.status.busy": "2022-10-25T22:59:04.537947Z",
          "iopub.status.idle": "2022-10-25T22:59:04.548587Z",
          "shell.execute_reply": "2022-10-25T22:59:04.547606Z",
          "shell.execute_reply.started": "2022-10-25T22:59:04.540035Z"
        },
        "trusted": true,
        "id": "gjlwumGzSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.argmax(ap)\n",
        "result = np.array(result).reshape(1)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:59:04.551484Z",
          "iopub.status.busy": "2022-10-25T22:59:04.55112Z",
          "iopub.status.idle": "2022-10-25T22:59:04.558454Z",
          "shell.execute_reply": "2022-10-25T22:59:04.55754Z",
          "shell.execute_reply.started": "2022-10-25T22:59:04.551446Z"
        },
        "trusted": true,
        "id": "IPkwmY0vSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "func.inverse_transform(result)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-25T22:59:04.560591Z",
          "iopub.status.busy": "2022-10-25T22:59:04.560094Z",
          "iopub.status.idle": "2022-10-25T22:59:04.56928Z",
          "shell.execute_reply": "2022-10-25T22:59:04.568114Z",
          "shell.execute_reply.started": "2022-10-25T22:59:04.560533Z"
        },
        "trusted": true,
        "id": "_L-7B8GuSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "gXmiZZ_LSh8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "a = input(\"Enter your statement: \")\n",
        "print(a)\n",
        "\n",
        "tokenize.fit_on_texts(a)\n",
        "max_length = 64\n",
        "vocab_size = len(tokenize.word_index) + 1\n",
        "a = pad_sequences(tokenize.texts_to_sequences(a), maxlen=max_length, padding=\"post\")\n",
        "a = tf.constant(a)\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in range(10):\n",
        "    try:\n",
        "\n",
        "        prediction = model.predict(a)\n",
        "        ap = []\n",
        "        for i in range(len(prediction)):\n",
        "            ap.append(np.argmax(prediction[i]))\n",
        "        np.argmax(ap)\n",
        "        result = np.argmax(ap)\n",
        "        result = np.array(result).reshape(1)\n",
        "        output.append(func.inverse_transform(result))\n",
        "    except:\n",
        "        print(\"Invalid input\")\n",
        "\n",
        "\n",
        "output"
      ],
      "metadata": {
        "id": "O5jiUcDFSh8K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4DLa6vxSh8K"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}